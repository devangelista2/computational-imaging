{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebcfc18a",
   "metadata": {},
   "source": [
    "# Cross-Domain End-to-End Reconstruction\n",
    "\n",
    "Despite the impressive performance (in terms of both reconstruction quality and efficiency) often achieved by convolution-based neural networks, their direct application in practice can be limited by a crucial constraint: their inherent structure typically requires the input data and the output solution to have matching dimensions. They struggle to process data where the dimensionality of the input datum $y$ differs significantly from the dimensionality of the desired solution $x$.\n",
    "\n",
    "Consider, for example, the popular UNet architecture. Due to its symmetric encoding-decoding structure, the dimensionality of the model's input remains unchanged at the output. This poses a challenge in applications such as Computed Tomography (CT) or Super-Resolution (SR), where the **domain** (and thus, often the dimensionality and structure) of the measured datum $y$ is inherently different from the **domain** of the desired solution image $x$.\n",
    "\n",
    "```{image} /imgs/CNN-issue.png\n",
    ":width: 800px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "## Resizing?\n",
    "\n",
    "A seemingly straightforward solution to this limitation is to **resize** the input datum $y$ so that its dimensions match the expected dimensions of the reconstruction $x$. This could be achieved using functions like `Resize()` from the `torchvision` package. While this modification allows a model like UNet to technically process the mapping from the resized $y$ to $x$, this approach has generally been shown to be ineffective in practice for many inverse problems.\n",
    "\n",
    "Indeed, convolution-based end-to-end models suffer from another limitation stemming from their core properties of locality and translation invariance (as discussed in a previous chapter). When information pertaining to the solution $x$ gets **spread** widely across the measurement $y$ (for instance, when the value of a pixel in $x$ influences many, potentially spatially distant, pixels in $y$), convolutional filters struggle to accurately reconstruct the solution. The local receptive field of convolutions cannot easily capture these non-local dependencies, even if the input and output shapes are forced to match via resizing.\n",
    "\n",
    "```{image} /imgs/CT_acquisition.png\n",
    ":width: 800px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "Consider the Computed Tomography (CT) inverse problem as an example. Here, the ground truth image $x_{GT}$ is processed by the CT forward projection operator $K$ to obtain a measurement $y = Kx_{GT}$ called a **sinogram**. Each pixel $y_{i, j}$ in the sinogram represents the line integral of $x_{GT}$ along a specific path (e.g., line $j$ at projection angle $i$). Clearly, not only are the shapes of $x_{GT}$ (a 2D or 3D spatial image) and $y$ (a sinogram with dimensions like number of angles $\\times$ number of detector bins, e.g., $(n_\\alpha, n_d)$) generally different, but locality is also lost. Each point in the sinogram $y$ depends on multiple, potentially distant, pixels in the original image $x_{GT}$. Consequently, a standard convolution-based model applied directly to the sinogram (even if resized) will likely be ineffective at reconstructing $x_{GT}$.\n",
    "\n",
    "## Pre-processing\n",
    "\n",
    "A classical and more robust solution to handle domain mismatch is to introduce a **pre-processing** step. The core idea is to apply an initial, often simple, reconstruction algorithm that maps the measurement $y$ back into the *domain* of the solution $x$, producing a coarse or approximate reconstruction $\\tilde{x}$. The quality of this initial reconstruction $\\tilde{x}$ is not the primary concern; its main purpose is to bridge the domain gap, providing an input that has the correct dimensionality and spatial structure for the subsequent neural network. The network then acts essentially as a **post-processing** or **refinement** layer, taking the coarse estimate $\\tilde{x}$ and producing the final, higher-quality reconstruction.\n",
    "\n",
    "To minimize computational overhead and avoid potential bottlenecks, this initial approximation step should ideally be very fast. A classic approach is to use the transposed forward operator $K^T$ (often related to back-projection in problems like CT) as a simple mapping:\n",
    "\n",
    "$$\n",
    "\\tilde{x} = K^T y.\n",
    "$$\n",
    "\n",
    "Clearly, $\\tilde{x}$ will generally be a low-quality image, as $K^T$ maps $y$ back to the image domain without explicitly optimizing for reconstruction fidelity. However, this transformation is often computationally inexpensive, making it suitable for this pre-processing role. It stands to reason that if a *better* initial reconstruction $\\tilde{x}$ could be obtained efficiently (without significantly increasing computational time), the subsequent neural network might achieve a better final result, thereby increasing the overall effectiveness of the pipeline.\n",
    "\n",
    "For this reason, more advanced pre-processing techniques have been developed, particularly in recent years. These methods are typically task-specific, as they often need to exploit the specific mathematical properties of the forward operator $K$ to yield a higher-quality initial estimate than simple transposition or back-projection. In the following sections, we might discuss specific pre-processing approaches for common inverse problems like CT and SR. For other inverse problems where specialized pre-processing methods are not readily available, recall that using the transposed operator $K^T$ always provides a basic, universally applicable pre-processing option.\n",
    "\n",
    "```{image} /imgs/preprocessing.png\n",
    ":width: 800px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc7d586",
   "metadata": {},
   "source": [
    "## A full pipeline to train end-to-end cross-domain UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47051c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available. CTProjector will use CPU.\n",
      "Attempting to create ASTRA projector type: 'linear' for 'parallel' geometry...\n",
      "Successfully created ASTRA projector type: 'linear'\n",
      "CTProjector initialized. Geometry: parallel. Using GPU: False. FBP Algorithm: FBP\n"
     ]
    }
   ],
   "source": [
    "#-----------------\n",
    "# This is just for rendering on the website\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "sys.path.append(\"..\")\n",
    "#-----------------\n",
    "\n",
    "from IPPy import operators, utilities, metrics, models\n",
    "from IPPy.nn import trainer, losses\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = utilities.get_device()\n",
    "\n",
    "# Define model\n",
    "model = models.UNet(ch_in=1, \n",
    "                    ch_out=1,\n",
    "                    middle_ch=[64, 128, 256],\n",
    "                    n_layers_per_block=2,\n",
    "                    down_layers=(\"ResDownBlock\", \"ResDownBlock\"),\n",
    "                    up_layers=(\"ResUpBlock\", \"ResUpBlock\"),\n",
    "                    final_activation=None).to(device)\n",
    "\n",
    "# Define dataset class\n",
    "class MayoDataset(Dataset):\n",
    "    def __init__(self, data_path, data_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.data_shape = data_shape\n",
    "\n",
    "        # We expect data_path to be like \"./data/Mayo/train\" or \"./data/Mayo/test\"\n",
    "        self.fname_list = glob.glob(f\"{data_path}/*/*.png\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fname_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load the idx's image from fname_list\n",
    "        img_path = self.fname_list[idx]\n",
    "\n",
    "         # To load the image as grey-scale\n",
    "        x = Image.open(img_path).convert(\"L\")\n",
    "\n",
    "        # Convert to numpy array -> (512, 512)\n",
    "        x = np.array(x) \n",
    "\n",
    "        # Convert to pytorch tensor -> (1, 512, 512) <-> (c, n_x, n_y)\n",
    "        x = torch.tensor(x).unsqueeze(0)\n",
    "\n",
    "        # Resize to the required shape\n",
    "        x = transforms.Resize(self.data_shape)(x) # (1, n_x, n_y)\n",
    "\n",
    "        # Normalize in [0, 1] range\n",
    "        x = (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "        return x\n",
    "\n",
    "# --- Load data\n",
    "train_data = MayoDataset(\"../data/Mayo/train\", data_shape=256)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True)\n",
    "\n",
    "# Define CTProjector operator\n",
    "K = operators.CTProjector(\n",
    "    img_shape=(256, 256),\n",
    "    angles=np.linspace(0, np.pi, 60),\n",
    "    det_size=512,\n",
    "    geometry=\"parallel\",\n",
    ")\n",
    "\n",
    "# --- Parameters\n",
    "n_epochs = 0\n",
    "\n",
    "loss_fn = losses.MixedLoss(\n",
    "    (nn.MSELoss(), losses.SSIMLoss(), losses.FourierLoss()),\n",
    "    (1, 0.1, 0.1),)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-4)\n",
    "\n",
    "# Cycle over the epochs\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # Cycle over the batches with tqdm\n",
    "    epoch_loss = 0.0\n",
    "    ssim_loss = 0.0\n",
    "    for t, x in enumerate(train_loader):\n",
    "        # Send x and y to device\n",
    "        x = x.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Compute associated y_delta\n",
    "            y = K(x)\n",
    "            y_delta = y + utilities.gaussian_noise(y, noise_level=0.01)\n",
    "\n",
    "            # --- PREPROCESSING\n",
    "            x_FBP = K.FBP(y_delta)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        x_pred = model(x_FBP)\n",
    "        loss = loss_fn(x_pred, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update loss\n",
    "        epoch_loss += loss.item()\n",
    "        ssim_loss += metrics.SSIM(x_pred.cpu().detach(), x.cpu().detach())\n",
    "\n",
    "        # Update tqdm bar\n",
    "        print(\n",
    "            {\n",
    "                \"Loss\": f\"{epoch_loss / (t + 1):.4f}\",\n",
    "                \"SSIM\": f\"{ssim_loss / (t + 1):.4f}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Save model every 5 epochs (overwrite)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        # Save model state\n",
    "        trainer.save(model, weights_path=\"../weights/CTUNet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
