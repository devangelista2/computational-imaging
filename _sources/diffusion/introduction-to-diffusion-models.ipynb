{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d06178a3",
   "metadata": {},
   "source": [
    "# Diffusion Models for Image Generation\n",
    "\n",
    "As previously introduced, in this final chapter we will discuss more in details the working mechanism of Diffusion Models, the state-of-the-art for image generation.\n",
    "\n",
    "## Introduction to Diffusion Models\n",
    "\n",
    "Diffusion models are a class of generative models that produce data samples by **iteratively denoising a Gaussian noise vector**. Unlike GANs or VAEs, they are **likelihood-based models** that combine principles from **non-equilibrium thermodynamics** and **probabilistic modeling**.\n",
    "\n",
    "The key idea is to define a **Markov chain** that slowly destroys the structure of data by adding Gaussian noise over several steps, and then train a neural network to **reverse this noising process**, recovering the original data.\n",
    "\n",
    "```{image} /imgs/diffusion-diagram.png\n",
    ":width: 800px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "### The Forward Process: Gradual Noising\n",
    "\n",
    "Let $x_0 \\sim p_{\\text{data}}(x)$ be a sample from the real data distribution.\n",
    "\n",
    "We define a sequence of latent variables $x_1, x_2, \\ldots, x_T$ where noise is added at each step according to a predefined variance schedule $ \\beta_1, \\dots, \\beta_T$. The forward (noising) process is defined as:\n",
    "\n",
    "$$\n",
    "q(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} \\, x_{t-1}, \\beta_t I)\n",
    "$$\n",
    "\n",
    "This is a **Markov process** that gradually transforms the data into pure noise.\n",
    "\n",
    "We can also write the marginal distribution of $x_t$ directly given $x_0$ as:\n",
    "\n",
    "$$\n",
    "q(x_t \\mid x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\alpha_t = 1 - \\beta_t, \\quad \\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s\n",
    "$$\n",
    "\n",
    "Intuitively, as $t \\to T$, $x_t$ becomes close to an isotropic Gaussian $\\mathcal{N}(0, I)$.\n",
    "\n",
    "### The Reverse Process: Learning to Denoise\n",
    "\n",
    "Our goal is to learn the **reverse-time process**:\n",
    "\n",
    "$$\n",
    "p_\\theta(x_{t-1} \\mid x_t)\n",
    "$$\n",
    "\n",
    "Unlike the forward process, this is **not known analytically**. We approximate it using a neural network parameterized by $\\theta$.\n",
    "\n",
    "Assuming a Gaussian form for the reverse conditional:\n",
    "\n",
    "$$\n",
    "p_\\theta(x_{t-1} \\mid x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))\n",
    "$$\n",
    "\n",
    "Most models fix $\\Sigma_\\theta$ and train the network to predict only the mean (or alternatively the noise that was added, as we'll see next).\n",
    "\n",
    "### Denoising Score Matching (Simplified View)\n",
    "\n",
    "Rather than modeling the full posterior $p_\\theta(x_{t-1} \\mid x_t)$, the training objective is simplified by using **denoising score matching**. The network learns to **predict the noise** $\\epsilon$ added at each step:\n",
    "\n",
    "$$\n",
    "x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "A neural network $\\epsilon_\\theta(x_t, t)$ is trained to minimize the expected squared error:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{simple}}(\\theta) = \\mathbb{E}_{x_0, \\epsilon, t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(x_t, t) \\right\\|^2 \\right]\n",
    "$$\n",
    "\n",
    "This formulation greatly simplifies training and leads to excellent sample quality.\n",
    "\n",
    "### Summary of Diffusion Model Structure\n",
    "\n",
    "- **Forward process**: adds small amounts of Gaussian noise step-by-step to data\n",
    "- **Reverse process**: learned by a neural network to denoise\n",
    "- **Final sample generation**: starts from pure Gaussian noise and applies the learned denoising steps\n",
    "\n",
    "This framework allows for **stable training**, unlike GANs, and for **high-quality image synthesis**, often outperforming VAEs and GANs in perceptual quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1631c8fd",
   "metadata": {},
   "source": [
    "## Training Diffusion Models\n",
    "\n",
    "Once we have defined the forward noising process and the reverse denoising model, the training phase consists in teaching the neural network to **predict the noise** that was added to a clean image.\n",
    "\n",
    "We use a **time-dependent neural network** $\\epsilon_\\theta(x_t, t)$ that receives as input a noisy image $x_t$ and a timestep $t$, and tries to estimate the noise $\\epsilon$ used to corrupt the original clean image $x_0$.\n",
    "\n",
    "### Recap: Sampling from the Forward Process\n",
    "\n",
    "Recall that we can sample a noisy image $x_t$ directly given the clean image $x_0$ as:\n",
    "\n",
    "$$\n",
    "x_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "This gives us a way to **synthetically generate training data pairs** \\((x_t, \\epsilon)\\) from a dataset of real images $x_0$.\n",
    "\n",
    "### The Loss Function\n",
    "\n",
    "The model is trained by minimizing the **mean squared error** between the predicted noise and the actual sampled noise:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{simple}}(\\theta) = \\mathbb{E}_{x_0, \\epsilon, t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(x_t, t) \\right\\|^2 \\right]\n",
    "$$\n",
    "\n",
    "This approach has the following advantages:\n",
    "- It avoids explicitly computing the reverse conditional distribution.\n",
    "- It is **simple to implement**.\n",
    "- It works well in practice.\n",
    "\n",
    "### Training Procedure in PyTorch\n",
    "\n",
    "Here is a minimal example of how this training step might look like using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310e60c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_alphas(beta_schedule):\n",
    "    beta = torch.tensor(beta_schedule)\n",
    "    alpha = 1.0 - beta\n",
    "    alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "    return alpha, alpha_bar\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Minimal UNet-like model for illustration\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 3, 3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Embed time as additional channel or via positional encoding\n",
    "        return self.net(x)\n",
    "\n",
    "# Assume we already have: x0: (B, C, H, W), sampled from data\n",
    "# t: timestep indices uniformly sampled in [1, T]\n",
    "def sample_xt(x0, t, alpha_bar):\n",
    "    sqrt_alpha_bar = torch.sqrt(alpha_bar[t])[:, None, None, None]\n",
    "    sqrt_one_minus_alpha_bar = torch.sqrt(1 - alpha_bar[t])[:, None, None, None]\n",
    "    noise = torch.randn_like(x0)\n",
    "    xt = sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * noise\n",
    "    return xt, noise\n",
    "\n",
    "# Training step\n",
    "def training_step(model, x0, t, alpha_bar, optimizer):\n",
    "    xt, noise = sample_xt(x0, t, alpha_bar)\n",
    "    pred_noise = model(xt, t)\n",
    "    loss = nn.MSELoss()(pred_noise, noise)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a4307a",
   "metadata": {},
   "source": [
    "## Image Generation\n",
    "\n",
    "Once the model has been trained to predict the noise added during the forward process, we can **generate new images** by reversing the diffusion process. This process begins from **pure Gaussian noise** and proceeds step-by-step, applying the learned denoising network.\n",
    "\n",
    "```{image} /imgs/DM.png\n",
    ":width: 800px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "### The Reverse Sampling Process\n",
    "\n",
    "Given a trained model $\\epsilon_\\theta(x_t, t)$, we start from $x_T \\sim \\mathcal{N}(0, I)$ and apply the following iterative update:\n",
    "\n",
    "$$\n",
    "x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right) + \\sigma_t z\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\alpha_t = 1 - \\beta_t$\n",
    "- $\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$\n",
    "- $z \\sim \\mathcal{N}(0, I)$ is fresh Gaussian noise\n",
    "- $\\sigma_t^2$ is typically set to $\\beta_t$, the variance of the forward process\n",
    "\n",
    "At each step, we use the network to predict the noise added to $x_t$, and subtract it out to get $x_{t-1}$, optionally adding some randomness (except at $t = 1$).\n",
    "\n",
    "### Pseudocode of the Sampling Process\n",
    "\n",
    "Here is a simplified description of the denoising loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14151f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, image_size, T, alpha, alpha_bar, beta):\n",
    "    device = \"cpu\" # Set device\n",
    "\n",
    "    x = torch.randn(1, 3, image_size, image_size).to(device)  # Start from pure noise\n",
    "\n",
    "    for t in reversed(range(1, T)):\n",
    "        t_tensor = torch.full((1,), t, dtype=torch.long).to(device)\n",
    "        epsilon_theta = model(x, t_tensor)\n",
    "\n",
    "        alpha_t = alpha[t]\n",
    "        alpha_bar_t = alpha_bar[t]\n",
    "        beta_t = beta[t]\n",
    "\n",
    "        coef1 = 1 / torch.sqrt(alpha_t)\n",
    "        coef2 = (1 - alpha_t) / torch.sqrt(1 - alpha_bar_t)\n",
    "\n",
    "        mean = coef1 * (x - coef2 * epsilon_theta)\n",
    "        if t > 1:\n",
    "            noise = torch.randn_like(x)\n",
    "            sigma = torch.sqrt(beta_t)\n",
    "            x = mean + sigma * noise\n",
    "        else:\n",
    "            x = mean  # Final step: no noise added\n",
    "\n",
    "    return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
