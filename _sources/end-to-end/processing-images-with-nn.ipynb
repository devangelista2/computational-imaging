{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing images with Neural Networks\n",
    "After introducing the concept of Neural Networks (in its simplest form, that is the MLP), we are now ready to move back to the main topic of this course: **image reconstruction**. In the following, we will quickly recall the problem setup, mainly to fix some notation, and we will discuss how the basic architecture of MLP can be modified to address the task of image reconstruction.\n",
    "This discussion will include the concept of **Convolutional Neural Networks (CNN)**, a particular modifications to the already-described MLP that allows for a more flexible processing of the input image, which generalizes better on new data by imitating the behavior of the human eye. To this aim, we will introduce the concept of **Receptive Field (RF)**, as the sub-portion of input image that provides the context in image reconstruction. To improve the RF, we will then discuss **U-Net**, arguably the most-used neural network architecture for every possible task related to images, highlighiting its main advantages and limitations.\n",
    "\n",
    "Finally, we will introduce **Vision Transformers (ViT)**: a recently-introduced architecture which seems to perform particularly well on computer vision tasks trying to imitate the success that Transformers already had in Language Processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Brief Recall on Image Reconstruction\n",
    "\n",
    "In the first module of this course, you already discussed the problem of image reconstruction from measurements acquired via **linear operators**. In particular, this was done by considering the following acquisition system:\n",
    "\n",
    "$$\n",
    "y^\\delta = Kx_{true} + e,\n",
    "$$\n",
    "\n",
    "where $K \\in \\mathbb{R}^{m \\times n}$ represents the **acquisition operator**, $x_{true} \\in \\mathbb{R}^n$ is the **true datum** we want to reconstruct (here represented in *vectorized* form), $e \\in \\mathbb{R}^m$ is the **measurement noise**, which satisfies $|| e ||_2 \\leq \\delta$, and $y^\\delta \\in \\mathbb{R}^m$ is the **acquired datum**.\n",
    "\n",
    "The task of image reconstruction is to approximate $x_{true}$ (denoted as $x^*$ in the following), starting from $y^\\delta$ and, possibly, some information about the noiseâ€”e.g., $e$ could be Gaussian noise with zero mean and a given standard deviation $\\sigma>0$.\n",
    "\n",
    "For simplicity, in the first part of this course, $x_{true}$ was usually represented as a matrix of shape $n_x \\times n_y$, where $n_x$ and $n_y$ denote the number of pixels per row and column, respectively. Clearly, this implies that $(n_x, n_y)$ satisfies $n_x \\cdot n_y = n$. Similarly, the acquired measurement data $y^\\delta$ is also treated as an image, with shape $m_x \\times m_y$ such that $m_x \\cdot m_y = m$.\n",
    "\n",
    "```{note}\n",
    "Due to the dimensions of the reconstructed image $x_{true}$, the operator $K$ **cannot** be stored in memory. For this reason, we typically consider an **operator** that *simulates* the application of $K$ to the input $x_{true}$.\n",
    "```\n",
    "\n",
    "We also recall that most classical methods for solving the image reconstruction problem defined above rely on the **regularized least squares optimization problem**:\n",
    "\n",
    "$$\n",
    "\\min_{x \\in \\mathcal{X}} \\frac{1}{2} || K x - y^\\delta ||_2^2 + \\lambda R(x),\n",
    "$$\n",
    "\n",
    "where $\\mathcal{X}$ denotes the image domain (typically, $\\mathcal{X} = \\{ x \\geq 0 \\}$), $R(x)$ is the regularizer, which incorporates prior information about the solution, and $\\lambda > 0$ is the *regularization parameter*.\n",
    "\n",
    "This optimization problem is then solved using an *optimizer*, which depends on the mathematical properties of $R(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks for Image Processing\n",
    "While working with neural network, we need to take a slightly different approach. Indeed, as we already remarked in the previous sections, a neural network pipeline is composed of two main components:\n",
    "* A **model architecture**, described by the type of layers, the number of layers ($L$) and the activation functions ($\\rho$), and represented as $f_\\Theta$ for simplicity.\n",
    "* A **training set** $D$ containing $N$ pairs of input-output data, which is used to train the model, optimizing its parameters to achieve the task described by $D$.\n",
    "\n",
    "While we leave the description of the neural network model to the next few sections, we will focus here on the on how the dataset is usually built to achieve image processing tasks.\n",
    "\n",
    "### End-to-end vs Hybrid\n",
    "The first distinction that is important to clarify when working with neural networks for image reconstruction is the **end-to-end** approach versus the **hybrid** approach. \n",
    "\n",
    "* An end-to-end neural network is a model $f_\\Theta$ that is trained to take as input the corrupted datum $y^\\delta$ and **directly** compute the reconstruction $x^* = f_\\Theta(y^\\delta)$, in a single forward application of the model. This causes ...\n",
    "\n",
    "```{image} /imgs/end-to-end.pdf\n",
    ":width: 600px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "* An hybrid algorithm for image reconstruction, instead, ...\n",
    "\n",
    "```{image} /imgs/hybrid-approach.pdf\n",
    ":width: 600px\n",
    ":align: center\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
