{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing images with Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first module of this course, you already discussed the problem of image reconstruction from measurements acquired via linear operators. In particular, we recall that this has been done by considering the following acquisition system:\n",
    "\n",
    "$$\n",
    "y^\\delta = Kx_{true} + e,\n",
    "$$\n",
    "\n",
    "where $K \\in \\mathbb{R}^{m \\times n}$ represents the acquisition operator, $x_{true} \\in \\mathbb{R}^n$ is the true datum we want to reconstruct (here indicated in vectorized form), $e \\in \\mathbb{R}^m$ is the measurement noise, which satisfies $|| e ||_2 \\leq \\delta$, and $y^\\delta \\in \\mathbb{R}^m$ is the acquired datum.\n",
    "\n",
    "The task of image reconstruction is to reconstruct an approximated version of $x_{true}$ (named $x^*$ in the following), starting from $y^\\delta$ and, possibly, some information on the noise-e.g. $e$ could be a Gaussian noise with zero mean and a given standard deviation $\\sigma>0$.\n",
    "\n",
    "For simplicity, in the first part of this course, you usually represented $x_{true}$ as a matrix with shape $n_x \\times n_y$, representing the number of pixels per row ($n_x$) and per column ($n_y$) in $x$. Clearly, this implies that $(n_x, n_y)$ satisfies $n_x \\cdot n_y = n$. Similarly, the acquired measurement data $y^\\delta$ is again an image, with shape $m_x \\times m_y$ so that $m_x \\cdot m_y = m$.\n",
    "\n",
    "```{note}\n",
    "Due to the dimension of the reconstructed image $x_{true}$, the operator $K$ **cannot** be stored in memory. For this reason, we usually consider an **operator** which *simulates* the application of $K$ to the input $x_{true}$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also recall that the majority of *classical* methods to solve the image reconstruction problem as defined above are based on the **regularized least square** optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{x \\in \\mathcal{X}} \\frac{1}{2} || Kx - y^\\delta ||_2^2 + \\lambda R(x),\n",
    "$$\n",
    "\n",
    "where $\\mathcal{X}$ is the space denoting the image domain (usually, $\\mathcal{X} = \\{ x \\geq 0 \\}$), $R(x)$ is the **regularizer**, which describes prior information on the solution, and $\\lambda > 0$ is the **regularization parameter**.\n",
    "\n",
    "This optimization method is then solved through an *optimizer*, based on the mathematical properties of $R(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module, we will follow a slightly different approach, as we will solve the inverse problem of recovering $x_{true}$ from $y^\\delta$ by employing **neural networks**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
