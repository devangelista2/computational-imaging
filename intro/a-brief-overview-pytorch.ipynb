{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A brief overview of PyTorch\n",
    "\n",
    "In this chapter, we will briefly introduce PyTorch: arguably the most used library for developing Neural Network models in Python. In particular, we will focus on few key components:\n",
    "\n",
    "- **Tensors:** Tensors are the building block of any pytorch model. Since we will largely use them, we need to at least learn their main properties and functionalities;\n",
    "- **Data:** Loading data in memory is a fundamental step in developing Neural Network-based models, and it requires special attention when the number of datapoints is large;\n",
    "- **Model Design:** A good model requires carefully optimizing its architecture (i.e. number of layers, number of neurons per layer, activation function, ...). In this chapter we will learn how to deploy a simple MLP network, and we will come back to architecture design later in the course;\n",
    "- **Training:** Training a model (i.e. optimizing its parameters to achieve the task described by the dataset) requires setting up a few basic components. In this chapter, we will learn how to train a neural network model on a fairly simple dataset, with the default choices of each component.\n",
    "\n",
    "```{note}\n",
    "While we will try to cover all the basics of Neural Network in this course, what described in the following is far from being a complete introduction to the topic. Please refer to the official [pytorch documentation](https://pytorch.org) or to any tutorial on Youtube for a more complete introduction.\n",
    "```\n",
    "\n",
    "## What is PyTorch?\n",
    "PyTorch is an **open-source deep learning framework** developed by Facebookâ€™s AI Research Lab (FAIR). It provides **tensor computation**, **automatic differentiation**, and **deep learning model building** capabilities with a user-friendly and Pythonic interface.\n",
    "\n",
    "PyTorch is widely used in both research and industry due to its **flexibility**, **ease of debugging**, and **strong community support**. It enables researchers and developers to quickly prototype and train neural networks using GPUs for acceleration.\n",
    "\n",
    "### Key Features of PyTorch\n",
    "- **Dynamic Computational Graphs**: Unlike TensorFlow 1.x, which relied on static graphs, PyTorch dynamically builds computational graphs, making it easier to debug and modify models.\n",
    "- **Automatic Differentiation (Autograd)**: PyTorch automatically computes gradients, making it seamless to implement **backpropagation** for neural networks (a topic which will be deeper explained later).\n",
    "- **GPU Acceleration**: PyTorch seamlessly integrates with CUDA and MPS (on Apple Silicon CPUs) for **fast GPU computing**.\n",
    "- **Strong Ecosystem**: Includes tools like `torchvision` for images, `torchtext` for NLP, and `torchaudio` for speech processing.\n",
    "\n",
    "### PyTorch vs. TensorFlow\n",
    "\n",
    "PyTorch and TensorFlow are the two most popular deep learning frameworks. Hereâ€™s a **comparison of their strengths and weaknesses**:\n",
    "\n",
    "| Feature           | PyTorch | TensorFlow |\n",
    "|------------------|---------|------------|\n",
    "| **Ease of Use** | Intuitive, Pythonic | More complex, requires more boilerplate |\n",
    "| **Dynamic Graphs** | âœ… Yes | ðŸš« No (TF 1.x), âœ… Yes (TF 2.x) |\n",
    "| **Debugging** | Easier (native Python debugging tools) | More difficult (static graphs in TF 1.x) |\n",
    "| **Performance** | Excellent for research and fast prototyping | Optimized for large-scale deployment |\n",
    "| **Ecosystem** | Torchvision, TorchText, TorchAudio | TensorFlow Hub, TF-Agents (RL), TensorFlow.js |\n",
    "| **Industry Adoption** | Preferred in research | Preferred in large-scale industry applications |\n",
    "| **Community Support** | Strong in academia and research | Larger enterprise-level adoption |\n",
    "\n",
    "### Which One Should You Choose?\n",
    "\n",
    "During your master degree, you will get in touch with both the frameworks described above. In particular, the course *Deep Learning* from professor Andrea Asperti will teach you Tensorflow, while we will use Pytorch in this course. However, you should:\n",
    "\n",
    "- **Choose PyTorch if**:\n",
    "  - You prioritize **ease of use** and fast prototyping.\n",
    "  - You work in **research** or academia.\n",
    "  - You need **dynamic graphs** for flexible model structures.\n",
    "- **Choose TensorFlow if**:\n",
    "  - You want **better production-ready tools** for deployment.\n",
    "  - You are working in **enterprise applications** with large-scale models.\n",
    "\n",
    "### Installation\n",
    "\n",
    "PyTorch provides an easy installation process. You can install it using `pip` (for Python users) or `conda` (for Anaconda users). It is sufficient to copy and paste the command from the official website: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/) by selecting your system preferences from the menu.\n",
    "\n",
    "```{note}\n",
    "I recommend to always use `pip` to install pytorch as it usually causes less issues.\n",
    "```\n",
    "\n",
    "Once installed, verify the installation by running the following command in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `torch` is installed correctly, it should print the version number and confirm whether CUDA is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Tensors\n",
    "At the core of Pytorch is the `tensor`, a multi-dimensional array similar to numpy arrays but with additional capabilities, such as GPU acceleration and automatic differentiation.\n",
    "\n",
    "Pytorch provides multiple ways to create tensors, most of which have the same syntax as numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Creating a tensor from a list\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "print(t1)\n",
    "\n",
    "# Creating a tensor with predefined values\n",
    "t2 = torch.zeros(3, 3)  # 3x3 matrix of zeros\n",
    "t3 = torch.ones(2, 4)   # 2x4 matrix of ones\n",
    "t4 = torch.rand(2, 2)   # 2x2 matrix of random values between 0 and 1\n",
    "\n",
    "print(t2)\n",
    "print(t3)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Properties\n",
    "\n",
    "Each `torch` tensor has several key attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape: {t.shape}\")  # Dimensions of the tensor\n",
    "print(f\"Data type: {t.dtype}\")  # Data type (default is float32)\n",
    "print(f\"Device: {t.device}\")  # CPU or GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Tensor Operations\n",
    "\n",
    "Tensors support element-wise operations, matrix multiplications, and reshaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "y = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "# Element-wise operations\n",
    "print(x + y)  # Addition\n",
    "print(x * y)  # Multiplication\n",
    "print(torch.sqrt(x.float()))  # Square root (requires float type)\n",
    "\n",
    "# Matrix multiplication\n",
    "print(x @ y)  # Equivalent to torch.matmul(x, y)\n",
    "\n",
    "# Reshaping tensors\n",
    "z = torch.arange(6).reshape(2, 3)\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Tensors to GPU\n",
    "\n",
    "**If** a GPU is available, we can move tensors to it for faster computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use GPU\n",
    "    x = x.to(device)\n",
    "    print(f\"Tensor is now on: {x.device}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoading\n",
    "\n",
    "In deep learning, we often work with *large datasets* that cannot fit into memory all at once. `torch` provides efficient tools to handle data loading through the `Dataset` and `DataLoader` classes.\n",
    "\n",
    "### The Dataset Class\n",
    "\n",
    "Pytorchâ€™s `torch.utils.data.Dataset` is an abstract class that must be subclassed to define custom datasets. A `Dataset` object should implement three methods:\n",
    "\n",
    "- `__init__`: Initializes the dataset (e.g., loads file paths, applies transformations).\n",
    "- `__len__`: Returns the total number of samples in the dataset.\n",
    "- `__getitem__`: Retrieves a single sample by index.\n",
    "\n",
    "**Creating a Custom Dataset:** Mathematically, a dataset is a sequence of pairs $\\{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\dots, (x^{(N)}, y^{(N)}) \\}$, where each $x^{(i)}$ is a $d$-dimensional vector, while $y^{(i)}$ is an $s$-dimensional vector.\n",
    "\n",
    "To create a dataset in `torch`, we need to build a dataset class so that, when it is called on index `i`, it returns the couple $(x^{(i)}, y^{(i)})$ as a `tuple` of tensors with shape `(d, )` and `(s, )`, called **input** and **output** shape, respectively.\n",
    "\n",
    "Sometimes (when the dimensionality of the data allows it), all the datapoints gets stacked together in two large tensors `X` and `Y`, which thus have shape `(N, d)` and `(N, s)`. Clearly, `X[i, :]` corresponds to the input tensor $x^{(i)}$, and `Y[i, :]` corresponds to the output tensor $y^{(i)}$.\n",
    "\n",
    "Consider, as an example, the dataset $D = \\{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\dots, (x^{(N)}, y^{(N)}) \\}$, such that $x^{(i)}$ are uniformly distributed datapoints in the range $[-2, 2]$, while $y^{(i)} = 2 x^{(i)} + 3$. In the following, we will avoid building `X` and `Y` explicitly, relying instead on definining a constructor that returns the pair $(x^{(i)}, y^{(i)})$ upon request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, N=100):\n",
    "        self.x = torch.linspace(-2, 2, N)\n",
    "        self.y = 2 * self.x + 3  # Linear function\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = SimpleDataset(N=200)\n",
    "\n",
    "# Fetch a single data point\n",
    "idx = 10\n",
    "x_sample, y_sample = dataset[idx]\n",
    "print(f\"x: {x_sample}, y: {y_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DataLoader Class\n",
    "\n",
    "We already observed that when the dataset is too large, sometimes it cannot be loaded into memory as a whole (expecially when working with GPU, which has usually lower dedicated memory compared to the system). On the other side, relying on single samples is usually too complex, as we would need more time to process the whole dataset.\n",
    "\n",
    "For this reason, when working with basically any Machine Learning algorithm (and in particular with neural networks), it is common to work with **mini-batches**.\n",
    "A minibatch is a subset of the dataset which contains a limited amount of memory, built by concatenating together multiple datapoints **randomly** extracted by the dataset. Usually, in Pytorch, a mini-batch (often called simply **batch**) is represented as a pair of tensors `(x, y)` with shapes `(b, d)` and `(b, s)`, respectively, where the **first** dimension represents the batch axis, where the number of elements `b` is called `batch_size`.  \n",
    "\n",
    "The operation of randomly sampled a given number of datapoints from the `Dataset` object in Pytorch is called a `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Iterate through batches\n",
    "for batch in dataloader:\n",
    "    x_batch, y_batch = batch\n",
    "    print(f\"Batch - x: {x_batch}, y: {y_batch}\")\n",
    "\n",
    "    break # For site impagination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Parameters of DataLoader:**\n",
    "\n",
    "- `batch_size`: Number of samples per batch.\n",
    "- `shuffle`: Whether to shuffle the data at the beginning of each epoch.\n",
    "- `num_workers`: Number of subprocesses to use for data loading (useful for large datasets).\n",
    "- `drop_last`: Whether to drop the last incomplete batch if dataset size isnâ€™t divisible by batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example: the California Housing Dataset\n",
    "\n",
    "Let's see an example on how to load a built-in dataset using `sklearn.datasets`. In particular, we we'll load we will use the **California Housing** dataset, which is a regression dataset where the goal is to predict house prices based on features such as median income, number of rooms, and population in an area. This dataset contains 8 numerical features (e.g., median income, total rooms, housing age, etc.) and one target variable (median house value in $100,000s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset from sklearn\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data\n",
    "\n",
    "Since neural networks work best with normalized inputs, we standardize the features using StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features for better training stability\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "y = y.reshape(-1, 1)  # Reshape target to be a column vector\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Custom PyTorch Dataset\n",
    "\n",
    "We define a custom dataset by subclassing torch.utils.data.Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaliforniaHousingDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = CaliforniaHousingDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = CaliforniaHousingDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, weâ€™ll define our first neural network model for classification using this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Our First Model in PyTorch\n",
    "\n",
    "Now that we understand how to load data, let's build a simple fully connected (dense) neural network using `torch.nn.Module`. We'll start with a basic model and then improve it step by step.\n",
    "\n",
    "### Defining a Simple Neural Network\n",
    "\n",
    "PyTorch models are created by subclassing `torch.nn.Module`. The key components are:\n",
    "\n",
    "- `__init__`: Defines the layers.\n",
    "- `forward`: Defines how data flows through the model.\n",
    "\n",
    "Let's create a simple Multi-Layer Perceptron (MLP) with one hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size) # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.fc1(x))  # Apply ReLU activation to first layer\n",
    "        x = self.fc2(x)          # Output layer (no activation for now)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = SimpleNN(input_size=8, \n",
    "                 hidden_size=64, \n",
    "                 output_size=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- `nn.Linear(input_size, hidden_size)`: Fully connected layer transforming the input.\n",
    "- `nn.ReLU()(x)`: Applies a non-linear activation function (ReLU activation).\n",
    "- `forward(x)`: Defines how the input data is processed.\n",
    "- No activation on the last layer: Typically, output activations depend on the task (e.g., `sigmoid` for binary classification, `softmax` for multi-class classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing Data Through the Model\n",
    "\n",
    "Given the model and the dataset, we can check its prediction over a random batch of datapoints (given by the `DataLoader`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data from the dataset\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "\n",
    "# Check the shape of the batch\n",
    "print(f\"Shape of x_batch: {x_batch.shape}. Shape of y_batch: {y_batch.shape}\")\n",
    "\n",
    "# Forward pass through the model\n",
    "y_prediction = model(x_batch)\n",
    "\n",
    "# Visualizing a value compared to the real (expected) solution\n",
    "print(f\"Real value: {y_batch[0].item()}. Model prediction: {y_prediction[0].item()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model\n",
    "\n",
    "You probably noticed that the model prediction is completely different from the real value of the target variable. This happens as the model has not been **trained** yet. As already remarked, training a model is the process of iteratively update its parameters $\\Theta$ so that it matches the training data.\n",
    "\n",
    "A neural network model is usually trained by a variant of the **Stochastic Gradient Descent (SGD)** algorithm: the stochastic version of the Gradient Descent optimization algorithm. In particular, given an initial value for the model parameters $\\Theta_0$, a loss function $\\ell: \\mathbb{R}^s \\times \\mathbb{R}^s \\to \\mathbb{R}_+$, and a training dataset $D$, the SGD algorithms iteratively update the parameters based on the following procedure:\n",
    "\n",
    "- Sample a batch $(x_b, y_b)$ from $D$.\n",
    "- Compute $g_k = \\nabla_{\\Theta} \\ell(f_{\\Theta_k}(x^{(i)}_b), y^{(i)}_b)$.\n",
    "- Update $\\Theta_{k+1} = \\Theta_k - \\nu g_k$.\n",
    "\n",
    "At this point, we already discussed how to create and sample a batch of data from $D$. The next step we need to learn is how to compute $g_k$, and here is where Pytorch becomes really useful.\n",
    "\n",
    "### Automatic Differentiation\n",
    "Pytorch tensors differs from numpy arrays mainly in that they keep track of each operations leading from a leaf tensor (i.e. a freshly created tensor) to the present tensor. This option (which is activated by default), can be modified by accessing the `requires_grad` property of the tensor.\n",
    "\n",
    "When a leaf tensor is declared with `requires_grad = True`, each operation involving it gets memorized. This way, it is possible to automatically compute the gradient of any function with respect to the leaf tensor by **backpropagating** from the output to the input via the computational graph, using the **chain rule** to combine the derivative at each step.\n",
    "\n",
    "Indeed, we recall that if $g: \\mathbb{R}^n \\to \\mathbb{R}^n$ is a function mapping a leaf tensor to an intermediate value $z = g(x)$, and $f: \\mathbb{R}^n \\to \\mathbb{R}$ is a scalar function (such as a loss function), mapping $z$ to an output $y = f(z)$, then the gradient of $f(g(x))$ with respect of $x$ can be easily computed as:\n",
    "\n",
    "$$\n",
    "\\nabla_x f(g(x)) = J_g(x) \\nabla_z f(z).\n",
    "$$\n",
    "\n",
    "This process is automatically performed in Pytorch by calling the `.backward()` method on any non-leaf tensor. The gradient with respect to $x$ can then be accessed by calling `x.grad`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a leaf tensor\n",
    "x = torch.linspace(0, 1, 20, requires_grad=True)\n",
    "\n",
    "# Compute y = x**2\n",
    "y = torch.square(x)\n",
    "\n",
    "# Compute loss = sum(x**2)\n",
    "loss = torch.sum(y)\n",
    "\n",
    "# Compute gradient of the loss\n",
    "loss.backward()\n",
    "\n",
    "# Extract gradient wrt x -> d/dx loss(x^**2) = 2*x\n",
    "g = x.grad\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a neural network\n",
    "This process can be exploited to run the Stochastic Gradient Descent (SGD) algorithm and train the neural network on our train loader.\n",
    "\n",
    "To do that, we should initialize an `optimizer` which keeps track of the gradient of the loss function with respect to the parameters $\\Theta$ when the `.backward()` method is called on the loss function, and it also applies the gradient descent step to the model parameters to update them.\n",
    "This is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function (for example, MSE)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Define optimizer (feeding the model parameters into it)\n",
    "# Adam -> variant of SGD algorithm commonly used nowadays\n",
    "#   lr -> \"learning rate\"\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-4)\n",
    "\n",
    "# Set other parameters (e.g. the number of epochs: number of times the training loop is repeated)\n",
    "n_epochs = 50\n",
    "\n",
    "# Epoch cycle\n",
    "for epoch in range(n_epochs):\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for k, data in enumerate(train_loader):\n",
    "        # Get x, y from data\n",
    "        x, y = data\n",
    "\n",
    "        # Send to device\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Compute neural network prediction\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # Compare y_pred with the real y\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        # Compute gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model weights\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad() # Reset the optimizer state: IMPORTANT\n",
    "\n",
    "        # Print out the avg value of the loss\n",
    "        # Commented for site impagination\n",
    "        # print(f\"Epoch: {epoch}. Avg Loss: {loss.item() / (k+1):0.4f}\", end=\"\\r\")\n",
    "    # print()\n",
    "\n",
    "# Saving the model after the cycle\n",
    "torch.save(model.state_dict(), \"path-for-model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the trained model\n",
    "Now that we optimized the neural network parameters, we are ready to check whether the prediction of the network on new data is good or not. To do that, we can simply load a batch from the test set, compute the prediction on it, and check if it matches the real value.\n",
    "\n",
    "To save memory, this operation can be done without tracking the gradient, as we won't use it to update the model weights. This is done by calling the operation in bewteen the `with torch.no_grad()` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable gradient memorization\n",
    "with torch.no_grad():\n",
    "    # Sample data from the dataset\n",
    "    x_batch, y_batch = next(iter(train_loader))\n",
    "\n",
    "    # Send to device\n",
    "    x_batch = x_batch.to(device)\n",
    "    y_batch = y_batch.to(device)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    y_prediction = model(x_batch)\n",
    "\n",
    "    print(f\"Prediction: {y_prediction[0].item():0.4f}. True: {y_batch[0].item():0.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is definitely better! Clearly, the prediction can be largely improved by optimizing all the parameters that we set up to this point. However, this is out of the scope of this course.\n",
    "\n",
    "We can now move to the next chapter, where we will learn how to actually reconstruct images with neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
